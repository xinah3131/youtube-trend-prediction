{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79cfa329",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "\n",
    "1. Find the best Modal (tuning)\n",
    "\n",
    "2. Exploratary Data Analysis\n",
    "\n",
    "3. Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebbc2dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "import joblib\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split,cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import TargetEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6fea09e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\LEGION\\Desktop\\MMU\\Data Science Fundamental\\Project\\Prediction of Video\\dataset\\merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85050d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'videoID', 'title', 'publishedAt', 'channelId',\n",
       "       'channelTitle', 'categoryId', 'tags', 'views', 'likes', 'comments',\n",
       "       'descriptionLength', 'thumbnailLink', 'dayOfWeek', 'daytime',\n",
       "       'duration', 'titleLength', 'questionMark', 'exclamationMark',\n",
       "       'sentimentScore', 'fullCapSentence', 'fullCapCount', 'trendingOrNot'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33ca8fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0','videoID','publishedAt','channelId','channelTitle','thumbnailLink','views','likes','comments','tags'],axis=1)\n",
    "df['categoryId'] = df['categoryId'].astype('category')\n",
    "df['dayOfWeek'] = df['dayOfWeek'].astype('category')\n",
    "df['daytime'] = df['daytime'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1261c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))  # set of English stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess(text,target_language='en'):\n",
    "    if not isinstance(text, str):\n",
    "        try:\n",
    "            text = str(text)\n",
    "        except:\n",
    "            raise TypeError('Input must be a string or a float')     \n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text) \n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Removing repeated characters\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    return words\n",
    "\n",
    "# Applying preprocessing function to title column\n",
    "df['cleanTitle'] = df['title'].apply(preprocess)\n",
    "df['cleanTitle'] = df['cleanTitle'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dee39999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'categoryId', 'descriptionLength', 'dayOfWeek', 'daytime',\n",
       "       'duration', 'titleLength', 'questionMark', 'exclamationMark',\n",
       "       'sentimentScore', 'fullCapSentence', 'fullCapCount', 'trendingOrNot',\n",
       "       'cleanTitle'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60c4430a",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "010fcc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['cleanTitle', 'categoryId', 'duration']\n",
      "Logistic Regression Mean Accuracy: 0.8222124782273056\n",
      "Features: ['cleanTitle', 'categoryId', 'duration']\n",
      "XGBoost Mean Accuracy: 0.8066133733785886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Selected Features: ['cleanTitle', 'categoryId', 'duration']\n",
      "Best Model Accuracy: 0.800750794109154\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.81      0.85     17289\n",
      "           1       0.62      0.77      0.69      6952\n",
      "\n",
      "    accuracy                           0.80     24241\n",
      "   macro avg       0.76      0.79      0.77     24241\n",
      "weighted avg       0.82      0.80      0.81     24241\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['cleanTitle', 'categoryId', 'duration', 'sentimentScore', 'questionMark', 'exclamationMark', 'fullCapSentence', 'fullCapCount']\n",
      "Logistic Regression Mean Accuracy: 0.8187636378837361\n",
      "Features: ['cleanTitle', 'categoryId', 'duration', 'sentimentScore', 'questionMark', 'exclamationMark', 'fullCapSentence', 'fullCapCount']\n",
      "XGBoost Mean Accuracy: 0.8061155196603806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Selected Features: ['cleanTitle', 'categoryId', 'duration', 'sentimentScore', 'questionMark', 'exclamationMark', 'fullCapSentence', 'fullCapCount']\n",
      "Best Model Accuracy: 0.7986881729301597\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.81      0.85     17289\n",
      "           1       0.62      0.78      0.69      6952\n",
      "\n",
      "    accuracy                           0.80     24241\n",
      "   macro avg       0.76      0.79      0.77     24241\n",
      "weighted avg       0.82      0.80      0.80     24241\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['cleanTitle', 'categoryId', 'duration', 'dayOfWeek', 'daytime']\n",
      "Logistic Regression Mean Accuracy: 0.8215991895994783\n",
      "Features: ['cleanTitle', 'categoryId', 'duration', 'dayOfWeek', 'daytime']\n",
      "XGBoost Mean Accuracy: 0.8068730941165011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Selected Features: ['cleanTitle', 'categoryId', 'duration', 'dayOfWeek', 'daytime']\n",
      "Best Model Accuracy: 0.7993482117074379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.81      0.85     17289\n",
      "           1       0.62      0.78      0.69      6952\n",
      "\n",
      "    accuracy                           0.80     24241\n",
      "   macro avg       0.76      0.79      0.77     24241\n",
      "weighted avg       0.82      0.80      0.81     24241\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['cleanTitle', 'categoryId', 'duration', 'sentimentScore', 'questionMark', 'exclamationMark', 'fullCapSentence', 'fullCapCount', 'dayOfWeek', 'daytime']\n",
      "Logistic Regression Mean Accuracy: 0.8187203816782805\n",
      "Features: ['cleanTitle', 'categoryId', 'duration', 'sentimentScore', 'questionMark', 'exclamationMark', 'fullCapSentence', 'fullCapCount', 'dayOfWeek', 'daytime']\n",
      "XGBoost Mean Accuracy: 0.8075801872086009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Selected Features: ['cleanTitle', 'categoryId', 'duration', 'sentimentScore', 'questionMark', 'exclamationMark', 'fullCapSentence', 'fullCapCount', 'dayOfWeek', 'daytime']\n",
      "Best Model Accuracy: 0.7999669980611361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.81      0.85     17289\n",
      "           1       0.62      0.77      0.69      6952\n",
      "\n",
      "    accuracy                           0.80     24241\n",
      "   macro avg       0.76      0.79      0.77     24241\n",
      "weighted avg       0.82      0.80      0.81     24241\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['cleanTitle', 'categoryId', 'duration', 'titleLength']\n",
      "Logistic Regression Mean Accuracy: 0.8219310801916595\n",
      "Features: ['cleanTitle', 'categoryId', 'duration', 'titleLength']\n",
      "XGBoost Mean Accuracy: 0.8093839819603736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Selected Features: ['cleanTitle', 'categoryId', 'duration', 'titleLength']\n",
      "Best Model Accuracy: 0.7983581535415205\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.81      0.85     17289\n",
      "           1       0.62      0.78      0.69      6952\n",
      "\n",
      "    accuracy                           0.80     24241\n",
      "   macro avg       0.76      0.79      0.77     24241\n",
      "weighted avg       0.82      0.80      0.80     24241\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['cleanTitle', 'categoryId', 'duration', 'descriptionLength']\n",
      "Logistic Regression Mean Accuracy: 0.8177463312368973\n",
      "Features: ['cleanTitle', 'categoryId', 'duration', 'descriptionLength']\n",
      "XGBoost Mean Accuracy: 0.8117288842931905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Selected Features: ['cleanTitle', 'categoryId', 'duration', 'descriptionLength']\n",
      "Best Model Accuracy: 0.7986469205065798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.81      0.85     17289\n",
      "           1       0.62      0.77      0.69      6952\n",
      "\n",
      "    accuracy                           0.80     24241\n",
      "   macro avg       0.76      0.79      0.77     24241\n",
      "weighted avg       0.82      0.80      0.80     24241\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['cleanTitle', 'categoryId', 'duration', 'descriptionLength', 'titleLength']\n",
      "Logistic Regression Mean Accuracy: 0.8187997220299973\n",
      "Features: ['cleanTitle', 'categoryId', 'duration', 'descriptionLength', 'titleLength']\n",
      "XGBoost Mean Accuracy: 0.8130131750992893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Selected Features: ['cleanTitle', 'categoryId', 'duration', 'descriptionLength', 'titleLength']\n",
      "Best Model Accuracy: 0.7963367847861061\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.80      0.85     17289\n",
      "           1       0.61      0.78      0.69      6952\n",
      "\n",
      "    accuracy                           0.80     24241\n",
      "   macro avg       0.76      0.79      0.77     24241\n",
      "weighted avg       0.82      0.80      0.80     24241\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['cleanTitle', 'titleLength', 'categoryId', 'descriptionLength', 'duration', 'sentimentScore', 'questionMark', 'exclamationMark', 'fullCapSentence', 'fullCapCount', 'dayOfWeek', 'daytime']\n",
      "Logistic Regression Mean Accuracy: 0.8174937901066188\n",
      "Features: ['cleanTitle', 'titleLength', 'categoryId', 'descriptionLength', 'duration', 'sentimentScore', 'questionMark', 'exclamationMark', 'fullCapSentence', 'fullCapCount', 'dayOfWeek', 'daytime']\n",
      "XGBoost Mean Accuracy: 0.8164331257408209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Selected Features: ['cleanTitle', 'titleLength', 'categoryId', 'descriptionLength', 'duration', 'sentimentScore', 'questionMark', 'exclamationMark', 'fullCapSentence', 'fullCapCount', 'dayOfWeek', 'daytime']\n",
      "Best Model Accuracy: 0.7974093477991832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.81      0.85     17289\n",
      "           1       0.62      0.78      0.69      6952\n",
      "\n",
      "    accuracy                           0.80     24241\n",
      "   macro avg       0.76      0.79      0.77     24241\n",
      "weighted avg       0.82      0.80      0.80     24241\n",
      "\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# Define a list of input features to test\n",
    "# Define the feature indices\n",
    "feature_indices = [\n",
    "    [0, 2, 4],                  # ['cleanTitle', 'categoryId', 'duration']\n",
    "    [0, 2, 4, 5, 6, 7, 8, 9],   # ['cleanTitle', 'categoryId', 'duration', 'sentimentScore', 'questionMark', 'exclamationMark', 'fullCapSentence', 'fullCapCount']\n",
    "    [0, 2, 4, 10, 11],          # ['cleanTitle', 'categoryId', 'duration', 'dayOfWeek', 'daytime']\n",
    "    [0, 2, 4, 5, 6, 7, 8, 9, 10, 11],   # ['cleanTitle', 'categoryId', 'duration', 'sentimentScore', 'questionMark', 'exclamationMark', 'fullCapSentence', 'fullCapCount', 'dayOfWeek', 'daytime']\n",
    "    [0, 2, 4, 1],               # ['cleanTitle', 'categoryId', 'duration', 'titleLength']\n",
    "    [0, 2, 4, 3],               # ['cleanTitle', 'categoryId', 'duration', 'descriptionLength']\n",
    "    [0, 2, 4, 3, 1],             # ['cleanTitle', 'categoryId', 'duration', 'descriptionLength', 'titleLength']\n",
    "    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]   # ['cleanTitle', 'titleLength', 'categoryId', 'descriptionLength', 'duration', 'sentimentScore', 'questionMark', 'exclamationMark', 'fullCapSentence', 'fullCapCount', 'dayOfWeek', 'daytime']\n",
    "]\n",
    "# Define the dictionary mapping indices to feature names\n",
    "feature_mapping = {\n",
    "    0: 'cleanTitle',\n",
    "    1: 'titleLength',\n",
    "    2: 'categoryId',\n",
    "    3: 'descriptionLength',\n",
    "    4: 'duration',\n",
    "    5: 'sentimentScore',\n",
    "    6: 'questionMark', \n",
    "    7: 'exclamationMark',\n",
    "    8: 'fullCapSentence',\n",
    "    9: 'fullCapCount',\n",
    "    10: 'dayOfWeek',\n",
    "    11: 'daytime'\n",
    "}\n",
    "\n",
    "# Generate the feature sets using the indices\n",
    "feature_sets = [[feature_mapping[i] for i in indices] for indices in feature_indices]\n",
    "\n",
    "# Define the target variable\n",
    "target_variable = 'trendingOrNot'\n",
    "\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('XGBoost', XGBClassifier())\n",
    "    # ('Random Forest', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "]\n",
    "\n",
    "for features in feature_sets:\n",
    "    X = df[features]\n",
    "    y = df[target_variable]\n",
    "    \n",
    "   # Define the column transformer based on the feature set\n",
    "    column_transformer = ColumnTransformer(transformers=[\n",
    "        ('text', TfidfVectorizer(min_df=1, stop_words='english'), 'cleanTitle')\n",
    "    ])\n",
    "    \n",
    "    if any(feature in ['titleLength', 'duration', 'descriptionLength','sentimentScore'] for feature in features):\n",
    "        numeric_features = [feat for feat in features if feat in ['titleLength', 'duration', 'descriptionLength','sentimentScore']]\n",
    "        column_transformer.transformers.append(('numeric', StandardScaler(), numeric_features))\n",
    "        \n",
    "    # if 'categoryId' in features:\n",
    "    #     column_transformer.transformers.append(('categorical', TargetEncoder(), ['categoryId']))\n",
    "\n",
    "    if any(feature in ['categoryId', 'dayOfWeek', 'daytime'] for feature in features):\n",
    "        categorical_features = [feat for feat in features if feat in ['categoryId', 'dayOfWeek', 'daytime']]\n",
    "        column_transformer.transformers.append(('categorical', TargetEncoder(), categorical_features))\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Oversample the training data\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Perform cross-validation and select the best model\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for name, model in models:\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', column_transformer),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        \n",
    "        # Perform 5-fold cross-validation\n",
    "        scores = cross_val_score(pipeline, X_train_resampled, y_train_resampled, cv=5)\n",
    "        \n",
    "        # Calculate the mean accuracy score\n",
    "        mean_score = scores.mean()\n",
    "        \n",
    "        # Print the results for the current model and feature set\n",
    "        print(f\"Features: {features}\")\n",
    "        print(f\"{name} Mean Accuracy: {mean_score}\")\n",
    "        \n",
    "        # Update the best model if necessary\n",
    "        if mean_score > best_score:\n",
    "            best_model = model\n",
    "            best_score = mean_score\n",
    "    \n",
    "    # Fit the best model on the resampled training data\n",
    "    best_pipeline = Pipeline([\n",
    "        ('preprocessor', column_transformer),\n",
    "        ('classifier', best_model)\n",
    "    ])\n",
    "    best_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Make predictions on the test set and evaluate the model\n",
    "    y_pred = best_pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"---------------------\")\n",
    "    print(\"Selected Features:\", features)\n",
    "    print(\"Best Model Accuracy:\", accuracy)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53e907a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "15 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.78765205        nan 0.82221248        nan 0.82746513]\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Best Score: 0.8274651276386548\n",
      "Best Parameters: {'classifier__C': 10.0, 'classifier__penalty': 'l2'}\n",
      "\n",
      "Model: XGBoost\n",
      "Best Score: 0.8232875059730287\n",
      "Best Parameters: {'classifier__learning_rate': 0.2, 'classifier__n_estimators': 300}\n",
      "\n",
      "Model: Random Forest\n",
      "Best Score: 0.9277840773414207\n",
      "Best Parameters: {'classifier__max_depth': None, 'classifier__n_estimators': 100}\n",
      "\n",
      "Accuracy: 0.84893362485046\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89     17289\n",
      "           1       0.73      0.75      0.74      6952\n",
      "\n",
      "    accuracy                           0.85     24241\n",
      "   macro avg       0.81      0.82      0.82     24241\n",
      "weighted avg       0.85      0.85      0.85     24241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the column transformer\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('text', TfidfVectorizer(min_df=1, stop_words='english'), 'cleanTitle'),\n",
    "    ('numeric', StandardScaler(), ['duration']),\n",
    "    ('categorical', TargetEncoder(), ['categoryId'])\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[['cleanTitle', 'categoryId','duration']]\n",
    "y = df['trendingOrNot']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Oversample the training data\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the pipeline with preprocessing and modeling steps\n",
    "pipelines = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('preprocessor', column_transformer),\n",
    "        ('classifier', LogisticRegression(random_state=42))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline([\n",
    "        ('preprocessor', column_transformer),\n",
    "        ('classifier', XGBClassifier(random_state=42))\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('preprocessor', column_transformer),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Define the parameter grids for each model\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__penalty': ['l1', 'l2']\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 5, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning and model comparison\n",
    "results = {}\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    param_grid = param_grids[model_name]\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    best_score = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'best_score': best_score,\n",
    "        'best_params': best_params\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "for model_name, result in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Best Score: {result['best_score']}\")\n",
    "    print(f\"Best Parameters: {result['best_params']}\")\n",
    "    print()\n",
    "\n",
    "# Fit the best model on the resampled training data\n",
    "best_model_name = max(results, key=lambda x: results[x]['best_score'])\n",
    "best_model = pipelines[best_model_name]\n",
    "best_params = results[best_model_name]['best_params']\n",
    "best_model.set_params(**best_params)\n",
    "best_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set and evaluate the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0bd6dd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Accuracy: 0.84893362485046\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89     17289\n",
      "           1       0.73      0.75      0.74      6952\n",
      "\n",
      "    accuracy                           0.85     24241\n",
      "   macro avg       0.81      0.82      0.82     24241\n",
      "weighted avg       0.85      0.85      0.85     24241\n",
      "\n",
      "Best Encoder: TargetEncoder\n",
      "Best Scaler: StandardScaler\n"
     ]
    }
   ],
   "source": [
    "from category_encoders import TargetEncoder, CountEncoder, CatBoostEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Define the column transformer\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('text', TfidfVectorizer(min_df=1, stop_words='english'), 'cleanTitle'),\n",
    "    ('numeric', StandardScaler(), ['duration']),\n",
    "    ('categorical', TargetEncoder(), ['categoryId'])\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[['cleanTitle', 'categoryId', 'duration']]\n",
    "y = df['trendingOrNot']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Oversample the training data\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the pipeline with preprocessing and modeling steps\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', column_transformer),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42))\n",
    "])\n",
    "\n",
    "# Define the parameter grid for encoders and scalers\n",
    "param_grid = {\n",
    "    'preprocessor__categorical': [TargetEncoder(), CountEncoder(), CatBoostEncoder()],\n",
    "    'preprocessor__numeric': [StandardScaler(), MinMaxScaler()]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best model and its evaluation on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Best Model Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Get the best encoder and scaler names\n",
    "best_encoder_name = grid_search.best_params_['preprocessor__categorical'].__class__.__name__\n",
    "best_scaler_name = grid_search.best_params_['preprocessor__numeric'].__class__.__name__\n",
    "\n",
    "print(\"Best Encoder:\", best_encoder_name)\n",
    "print(\"Best Scaler:\", best_scaler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column transformer\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('text', TfidfVectorizer(min_df=1, stop_words='english'), 'cleanTitle'),\n",
    "    ('numeric', StandardScaler(), ['duration']),\n",
    "    ('categorical',TargetEncoder(),['categoryId'])\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[['cleanTitle', 'categoryId','duration']]\n",
    "y = df['trendingOrNot']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Oversample the training data\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the pipeline with preprocessing and modeling steps\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', column_transformer),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the resampled training data\n",
    "pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set and evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11f10590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['85pct(new).pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "joblib.dump(pipeline, '85pct(new).pkl')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8d02c0a",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85e852a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def get_video_id(url):\n",
    "    video_id = None\n",
    "    parsed_url = urlparse(url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "\n",
    "    if parsed_url.netloc == 'youtu.be':\n",
    "        video_id = parsed_url.path[1:]\n",
    "    elif parsed_url.netloc in ('www.youtube.com', 'youtube.com'):\n",
    "        if 'v' in query_params:\n",
    "            video_id = query_params['v'][0]\n",
    "    return video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "69f1690f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>duration</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "      <th>category_id</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>cleanTitle</th>\n",
       "      <th>titleLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Incredible CREEP Blind Auditions in The Voice ...</td>\n",
       "      <td>These amazing kids sing Radioheads' Queen in T...</td>\n",
       "      <td>The Voice Global</td>\n",
       "      <td>2021-01-29T15:00:13Z</td>\n",
       "      <td>652.0</td>\n",
       "      <td>679890</td>\n",
       "      <td>11049</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>https://i.ytimg.com/vi/V1CIv_Cg_Qg/default.jpg</td>\n",
       "      <td>incredible creep blind audition voice kid</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Incredible CREEP Blind Auditions in The Voice ...   \n",
       "\n",
       "                                         description     channel_title  \\\n",
       "0  These amazing kids sing Radioheads' Queen in T...  The Voice Global   \n",
       "\n",
       "           publish_date  duration   views  likes comments category_id  \\\n",
       "0  2021-01-29T15:00:13Z     652.0  679890  11049        0          24   \n",
       "\n",
       "                                   thumbnail_link  \\\n",
       "0  https://i.ytimg.com/vi/V1CIv_Cg_Qg/default.jpg   \n",
       "\n",
       "                                  cleanTitle  titleLength  \n",
       "0  incredible creep blind audition voice kid           50  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import isodate\n",
    "\n",
    "# Set up the YouTube Data API client\n",
    "api_keys = ['AIzaSyC4hp-RHBw5uY4NcthYw-A2fqYyrG22kaE',\n",
    "'AIzaSyC7KzwigUsNJ4KNvqGfPqXVK9QcDBsKU78',\n",
    "'AIzaSyDEPBCb1PhEaYHuBgzW6D5-ldTHUCowuq4',\n",
    "'AIzaSyD-LN8Z7xG8OHtMQ89GRDvIaRQwkVHzfEo',\n",
    "'AIzaSyCW5J_uI37UPmq3mJVAhVdWNdGSMAMg5tI',\n",
    "'AIzaSyC8VVO0DhDY91lfyqqaUW85VKriqBiahBA',\n",
    "'AIzaSyDC744JL3Xa3eORSxORoxKpunKFPPMGb3Y',\n",
    "'AIzaSyA-DwJmtgWFO-I-Dwv1hcISJKXGDjbpZok',\n",
    "'AIzaSyDC744JL3Xa3eORSxORoxKpunKFPPMGb3Y',\n",
    "'AIzaSyD74KqDih_2AyOIJV-HaIvU9DdUOIyRONs',\n",
    "'AIzaSyALgq5vR27iGsuFuLiz-Ry4NGy6E-L1PUY',\n",
    "'AIzaSyC4hp-RHBw5uY4NcthYw-A2fqYyrG22kaE']\n",
    "current_key_index = 0\n",
    "\n",
    "def get_next_api_key():\n",
    "    global current_key_index\n",
    "    current_key_index = (current_key_index + 1) % len(api_keys)\n",
    "    return api_keys[current_key_index]\n",
    "def get_video_metadata(video_id):\n",
    "    try:\n",
    "        # Get the next API key\n",
    "        api_key = get_next_api_key()\n",
    "\n",
    "        # Set up the YouTube Data API client\n",
    "        youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "        # Call the API to retrieve video metadata\n",
    "        response = youtube.videos().list(\n",
    "            part='snippet,contentDetails,statistics',\n",
    "            id=video_id\n",
    "        ).execute()\n",
    "\n",
    "        # Extract the relevant metadata\n",
    "        if 'items' in response and len(response['items']) > 0:\n",
    "            video = response['items'][0]\n",
    "            metadata = {\n",
    "                'title': video['snippet']['title'],\n",
    "                'description': video['snippet']['description'],\n",
    "                'channel_title': video['snippet']['channelTitle'],\n",
    "                'publish_date': video['snippet']['publishedAt'],\n",
    "                'duration': video['contentDetails']['duration'],\n",
    "                'views': video['statistics']['viewCount'],\n",
    "                'likes': video['statistics'].get('likeCount', 0),\n",
    "                'comments': video['statistics'].get('commentCount', 0),\n",
    "                'category_id': video['snippet']['categoryId'],\n",
    "                'thumbnail_link': video['snippet']['thumbnails']['default']['url']\n",
    "            }\n",
    "            return metadata\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "video_id = get_video_id(\"https://www.youtube.com/watch?v=V1CIv_Cg_Qg\")\n",
    "metadata = get_video_metadata(video_id)\n",
    "\n",
    "# Create a DataFrame from the metadata\n",
    "data = pd.DataFrame([metadata])\n",
    "data['duration'] = data['duration'].apply(lambda x: isodate.parse_duration(x).total_seconds())\n",
    "data['cleanTitle'] = data['title'].apply(preprocess)\n",
    "data['cleanTitle'] = data['cleanTitle'].apply(lambda x: ' '.join(x))\n",
    "data['titleLength'] = data['title'].apply(lambda x: len(x))\n",
    "data['descriptionLength'] : data['description'].apply(lambda x: len(x))\n",
    "# Display the DataFrame\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5af5c18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52 0.48]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.76, 0.24]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the saved pipeline\n",
    "pipeline2 = joblib.load('85pct.pkl')\n",
    "\n",
    "test = data[['cleanTitle','category_id','duration']]\n",
    "test = test.rename(columns={'category_id': 'categoryId'})\n",
    "test['categoryId'] = test['categoryId'].astype('category')\n",
    "print(pipeline.predict_proba(test))\n",
    "test = data[['cleanTitle','titleLength','category_id','duration']]\n",
    "test = test.rename(columns={'category_id': 'categoryId'})\n",
    "test['categoryId'] = 29\n",
    "test['categoryId'] = test['categoryId'].astype('category')\n",
    "pipeline2.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "95969631",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = joblib.load('85pct.pkl')\n",
    "# Define the new data point\n",
    "new_title = \"Intelligent AI Chatbot in Python\"\n",
    "clean_new_title = preprocess(new_title)\n",
    "# Join the preprocessed words back into a string\n",
    "clean_new_title_str = ' '.join(clean_new_title)\n",
    "\n",
    "data1 = {\n",
    "    'cleanTitle': [clean_new_title_str],\n",
    "    'titleLength': [32],\n",
    "    'categoryId': [28],\n",
    "    'duration': [2142.0]\n",
    "}\n",
    "\n",
    "test2 = pd.DataFrame(data1)\n",
    "test2['categoryId'] = test2['categoryId'].astype('category')\n",
    "pipeline.predict(test2)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "688dc193",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e54e538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "descriptionLength    0.024141\n",
       "duration             0.021582\n",
       "titleLength          0.020980\n",
       "sentimentScore       0.016805\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mutual Information \n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df[['duration','titleLength','descriptionLength','sentimentScore']]\n",
    "y = df['trendingOrNot']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,y,test_size=0.3,random_state=0\n",
    ")\n",
    "mutual_info = mutual_info_classif(X_train,y_train)\n",
    "mutual_info = pd.Series(mutual_info)\n",
    "mutual_info.index = X_train.columns\n",
    "mutual_info.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56a16135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoryId         0.000000e+00\n",
       "daytime            5.508567e-35\n",
       "dayOfWeek          3.568344e-17\n",
       "questionMark       1.194363e-20\n",
       "exclamationMark    1.769994e-29\n",
       "fullCapSentence    5.243778e-07\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chisquare Test For Feature Selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(df[['categoryId', 'daytime', 'dayOfWeek','questionMark','exclamationMark','fullCapSentence']],\n",
    "                                              df['trendingOrNot'],test_size=0.3,random_state=100)\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "f_p_values=chi2(X_train,y_train)\n",
    "p_values=pd.Series(f_p_values[1])\n",
    "p_values.index=X_train.columns\n",
    "p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c4cc990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9244881527490223\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.88      0.92     17377\n",
      "           1       0.89      0.96      0.93     17399\n",
      "\n",
      "    accuracy                           0.92     34776\n",
      "   macro avg       0.93      0.92      0.92     34776\n",
      "weighted avg       0.93      0.92      0.92     34776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "# vectorizer_filename = 'vectorizer.sav'\n",
    "# with open(vectorizer_filename, 'rb') as f:\n",
    "#     vectorizer = pickle.load(f)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1,stop_words='english')\n",
    "X_title = vectorizer.fit_transform(df['cleanTitle'])\n",
    "\n",
    "# Load the selector_title from file\n",
    "# selector_title_filename = 'selector_title.sav'\n",
    "# with open(selector_title_filename, 'rb') as f:\n",
    "#     selector_title = pickle.load(f)\n",
    "# Load the saved model from a file\n",
    "# Extract features using TF-IDF\n",
    "\n",
    "\n",
    "# Select the top 1000 features based on chi-squared test\n",
    "# X_title = selector_title.fit_transform(X_title, df['trendingOrNot'])\n",
    "\n",
    "# Convert 'categoryId' and 'daysOfUploading' to a numpy array\n",
    "# X_numeric = df[['categoryId','title_length','duration']].to_numpy()\n",
    "# Scale 'title_length' and 'duration' features\n",
    "scaler = StandardScaler()\n",
    "X_numeric = scaler.fit_transform(df[['titleLength', 'duration']].to_numpy())\n",
    "\n",
    "# Include 'categoryId' as a feature\n",
    "X_cat = df[['categoryId']].to_numpy()\n",
    "\n",
    "# Concatenate the selected features and convert to a numpy array\n",
    "X = hstack([X_title, X_numeric, X_cat])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "y = df['trendingOrNot']\n",
    "# smote = SMOTE()\n",
    "# X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the SGD classifier model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set and evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b9f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories = df['categoryId'].unique()\n",
    "\n",
    "# Get the embeddings for each unique category\n",
    "category_embeddings = []\n",
    "for category in unique_categories:\n",
    "    embedding = model.predict(np.array([[category]]))\n",
    "    category_embeddings.append(embedding[0])\n",
    "    \n",
    "# Create a new dataframe with the category IDs and their corresponding embeddings\n",
    "embeddings_df = pd.DataFrame({'categoryId': unique_categories, 'embedding': category_embeddings})\n",
    "embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef60fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "\n",
    "\n",
    "# Define the numerical and categorical features\n",
    "numerical_features = ['sentimentScore']\n",
    "categorical_features = ['categoryId','dayOfWeek']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[['categoryId', 'sentimentScore','dayOfWeek']]\n",
    "y = df['trendingOrNot']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing pipelines for the numerical and categorical data\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "# Fit the numerical transformer on the training data\n",
    "numerical_transformer.fit(X_train[numerical_features])\n",
    "\n",
    "# Transform the training and testing numerical data\n",
    "X_train_numerical = numerical_transformer.transform(X_train[numerical_features])\n",
    "X_test_numerical = numerical_transformer.transform(X_test[numerical_features])\n",
    "\n",
    "# Define the input layers for the numerical and categorical data\n",
    "num_inputs = Input(shape=(X_train_numerical.shape[1],), name='num')\n",
    "cat_inputs = Input(shape=(1,), name='cat')\n",
    "\n",
    "# Define the embedding layer for the categorical data\n",
    "cat_embed = Embedding(input_dim=15, output_dim=15, name='cat_embed')(cat_inputs)\n",
    "cat_flat = Flatten()(cat_embed)\n",
    "\n",
    "# Concatenate the numerical and categorical inputs\n",
    "concatenated = Concatenate()([num_inputs, cat_flat])\n",
    "\n",
    "# Define the dense layers for the model\n",
    "dense_layer_1 = Dense(units=64, activation='relu')(concatenated)\n",
    "dense_layer_2 = Dense(units=32, activation='relu')(dense_layer_1)\n",
    "output_layer = Dense(units=1, activation='sigmoid')(dense_layer_2)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[num_inputs, cat_inputs], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train_numerical, X_train[categorical_features].values], y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_test_numerical, X_test[categorical_features].values], y_test)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d345e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 46431 is out of bounds for axis 0 with size 46430",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LEGION\\Desktop\\MMU\\Data Science Fundamental\\Project\\Prediction of Video\\Prediction.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LEGION/Desktop/MMU/Data%20Science%20Fundamental/Project/Prediction%20of%20Video/Prediction.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m ind \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(probs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LEGION/Desktop/MMU/Data%20Science%20Fundamental/Project/Prediction%20of%20Video/Prediction.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m good_words \u001b[39m=\u001b[39m words[ind[:\u001b[39m100\u001b[39m]]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/LEGION/Desktop/MMU/Data%20Science%20Fundamental/Project/Prediction%20of%20Video/Prediction.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m bad_words \u001b[39m=\u001b[39m words[ind[\u001b[39m-\u001b[39;49m\u001b[39m100\u001b[39;49m:]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LEGION/Desktop/MMU/Data%20Science%20Fundamental/Project/Prediction%20of%20Video/Prediction.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m good_prob \u001b[39m=\u001b[39m probs[ind[:\u001b[39m100\u001b[39m]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LEGION/Desktop/MMU/Data%20Science%20Fundamental/Project/Prediction%20of%20Video/Prediction.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m bad_prob \u001b[39m=\u001b[39m probs[ind[\u001b[39m-\u001b[39m\u001b[39m100\u001b[39m:]]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 46431 is out of bounds for axis 0 with size 46430"
     ]
    }
   ],
   "source": [
    "words = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "x = np.eye(X_test.shape[1])\n",
    "probs = clf.predict_log_proba(x)[:, 0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "good_words = words[ind[:100]]\n",
    "bad_words = words[ind[-100:]]\n",
    "\n",
    "good_prob = probs[ind[:100]]\n",
    "bad_prob = probs[ind[-100:]]\n",
    "\n",
    "print(\"Good words\\t     P(good | word)\")\n",
    "for w, p in zip(good_words, good_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "print(\"Bad words\\t     P(good | word)\")\n",
    "for w, p in zip(bad_words, bad_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62207024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#the grid of parameters to search over\n",
    "alphas = [.1, 1, 5]\n",
    "min_dfs = [1]\n",
    "\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_alpha = None\n",
    "best_min_df = None\n",
    "maxscore=-np.inf\n",
    "for alpha in alphas:\n",
    "    for min_df in min_dfs:\n",
    "        vectorizer = CountVectorizer(min_df = min_df)\n",
    "        Xthis, ythis = make_xy(df, vectorizer)\n",
    "        Xtrainthis=Xthis[mask]\n",
    "        ytrainthis=ythis[mask]\n",
    "        clf = MultinomialNB(alpha=alpha)\n",
    "        cvscore = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "\n",
    "        if cvscore > maxscore:\n",
    "            maxscore = cvscore\n",
    "            best_alpha, best_min_df = alpha, min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32090cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.875608\n",
      "Accuracy on test data:     0.768460\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=best_min_df)\n",
    "X, y = make_xy(df, vectorizer)\n",
    "xtrain=X[mask]\n",
    "ytrain=y[mask]\n",
    "xtest=X[~mask]\n",
    "ytest=y[~mask]\n",
    "\n",
    "clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "print(\"Accuracy on training data: {:2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae84e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good words\t     P(good | word)\n",
      "                2022 0.99\n",
      "              shorts 0.99\n",
      "          highlights 0.98\n",
      "                2021 0.98\n",
      "           minecraft 0.98\n",
      "             oficial 0.97\n",
      "                2023 0.96\n",
      "                 ufc 0.96\n",
      "               among 0.95\n",
      "                  mv 0.95\n",
      "                 lil 0.95\n",
      "            survived 0.95\n",
      "            hardcore 0.95\n",
      "               built 0.94\n",
      "         hermitcraft 0.94\n",
      "                 nba 0.94\n",
      "                 000 0.94\n",
      "                 nbc 0.94\n",
      "              lakers 0.93\n",
      "                 bts 0.93\n",
      "                  24 0.92\n",
      "                  fc 0.92\n",
      "               music 0.92\n",
      "              reacts 0.92\n",
      "              tiktok 0.92\n",
      "              funkin 0.91\n",
      "                 cbs 0.91\n",
      "                ring 0.91\n",
      "            pregnant 0.91\n",
      "               elden 0.91\n",
      "           breakdown 0.91\n",
      "           challenge 0.91\n",
      "              roblox 0.90\n",
      "             genshin 0.90\n",
      "                 snl 0.90\n",
      "          manchester 0.90\n",
      "              reveal 0.90\n",
      "                fifa 0.90\n",
      "               lyric 0.90\n",
      "                guys 0.90\n",
      "                espn 0.90\n",
      "               wings 0.90\n",
      "                 pov 0.90\n",
      "            youngboy 0.90\n",
      "               dream 0.90\n",
      "              golazo 0.89\n",
      "              league 0.89\n",
      "             premier 0.89\n",
      "                 100 0.89\n",
      "                 him 0.89\n",
      "               spicy 0.89\n",
      "                 ksi 0.89\n",
      "              bought 0.89\n",
      "               debut 0.89\n",
      "                durk 0.89\n",
      "                 psg 0.89\n",
      "                 smp 0.89\n",
      "           barcelona 0.89\n",
      "            playtime 0.89\n",
      "           liverpool 0.89\n",
      "             sidemen 0.89\n",
      "               hours 0.88\n",
      "              united 0.88\n",
      "                 but 0.88\n",
      "               방탄소년단 0.88\n",
      "               poppy 0.88\n",
      "               queen 0.88\n",
      "                baby 0.88\n",
      "                loss 0.88\n",
      "               blind 0.88\n",
      "              amelio 0.88\n",
      "          basketball 0.88\n",
      "               twins 0.88\n",
      "                full 0.87\n",
      "               spent 0.87\n",
      "                  he 0.87\n",
      "                nets 0.87\n",
      "                 ucl 0.87\n",
      "             manhunt 0.87\n",
      "                2020 0.87\n",
      "         performance 0.87\n",
      "                 sec 0.87\n",
      "              billie 0.87\n",
      "                 tnt 0.87\n",
      "                seek 0.87\n",
      "            birthday 0.87\n",
      "               drake 0.87\n",
      "                went 0.87\n",
      "                 usa 0.87\n",
      "              sports 0.86\n",
      "               squid 0.86\n",
      "             cooking 0.86\n",
      "              eilish 0.86\n",
      "          undisputed 0.86\n",
      "               dixie 0.86\n",
      "                 mod 0.86\n",
      "                 cat 0.86\n",
      "                food 0.86\n",
      "            extended 0.86\n",
      "                  ft 0.86\n",
      "Bad words\t     P(good | word)\n",
      "                leak 0.22\n",
      "             station 0.21\n",
      "                  e3 0.21\n",
      "             expanse 0.21\n",
      "            giveaway 0.21\n",
      "                 anh 0.21\n",
      "                 cho 0.21\n",
      "           transport 0.21\n",
      "               lapse 0.21\n",
      "            probably 0.21\n",
      "                tale 0.21\n",
      "              gaming 0.20\n",
      "             thrones 0.20\n",
      "             tractor 0.20\n",
      "          discussion 0.20\n",
      "              hubble 0.20\n",
      "           tailosive 0.20\n",
      "      reorchestrated 0.20\n",
      "               movie 0.20\n",
      "            handmaid 0.20\n",
      "               games 0.20\n",
      "                play 0.20\n",
      "                 ios 0.20\n",
      "                 suv 0.20\n",
      "             english 0.20\n",
      "              gerard 0.20\n",
      "              driver 0.20\n",
      "             fallout 0.19\n",
      "                 360 0.19\n",
      "             angular 0.19\n",
      "                cách 0.19\n",
      "              beyond 0.19\n",
      "               madea 0.18\n",
      "                 sun 0.18\n",
      "         predictions 0.18\n",
      "            illenium 0.18\n",
      "               perry 0.17\n",
      "           streaming 0.17\n",
      "                tech 0.17\n",
      "        construction 0.17\n",
      "              review 0.17\n",
      "              gamers 0.17\n",
      "                2013 0.17\n",
      "           astronomy 0.17\n",
      "                  tv 0.17\n",
      "                2010 0.16\n",
      "                 buy 0.16\n",
      "             harvard 0.15\n",
      "                 iss 0.15\n",
      "                 101 0.15\n",
      "               cargo 0.15\n",
      "             batería 0.14\n",
      "                 top 0.14\n",
      "               space 0.14\n",
      "       international 0.14\n",
      "                 4x4 0.14\n",
      "               touch 0.14\n",
      "                  3d 0.14\n",
      "              indeed 0.13\n",
      "                tame 0.13\n",
      "           discovery 0.13\n",
      "              undead 0.13\n",
      "                nasa 0.13\n",
      "                wait 0.13\n",
      "          statistics 0.13\n",
      "              nooner 0.13\n",
      "                  và 0.12\n",
      "             evolved 0.12\n",
      "           simulator 0.11\n",
      "           reactions 0.11\n",
      "                 hls 0.11\n",
      "                rant 0.11\n",
      "                drum 0.11\n",
      "                clip 0.11\n",
      "                 ark 0.11\n",
      "                sale 0.11\n",
      "             driving 0.10\n",
      "                trek 0.10\n",
      "             madonna 0.10\n",
      "           anonymous 0.10\n",
      "           paramount 0.09\n",
      "                  hd 0.08\n",
      "                hulu 0.08\n",
      "                 ps4 0.08\n",
      "          featurette 0.08\n",
      "              mobile 0.08\n",
      "                2015 0.07\n",
      "                spot 0.06\n",
      "             netflix 0.06\n",
      "            pictures 0.06\n",
      "             offroad 0.05\n",
      "                rent 0.05\n",
      "                  qn 0.04\n",
      "                 bảo 0.04\n",
      "                bình 0.04\n",
      "                2016 0.03\n",
      "                2019 0.03\n",
      "                2018 0.03\n",
      "             android 0.03\n",
      "                2017 0.03\n"
     ]
    }
   ],
   "source": [
    "words = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "x = np.eye(xtest.shape[1])\n",
    "probs = clf.predict_log_proba(x)[:, 0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "good_words = words[ind[:100]]\n",
    "bad_words = words[ind[-100:]]\n",
    "\n",
    "good_prob = probs[ind[:100]]\n",
    "bad_prob = probs[ind[-100:]]\n",
    "\n",
    "print(\"Good words\\t     P(good | word)\")\n",
    "for w, p in zip(good_words, good_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "print(\"Bad words\\t     P(good | word)\")\n",
    "for w, p in zip(bad_words, bad_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ee10b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46509481, 0.53490519]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(vectorizer.transform(['']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38726d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df[['title']]\n",
    "y = df.trendingOrNot\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.2)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(os_data_X, os_data_y,test_size = 0.2)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_clf = Pipeline([('tfidf',TfidfVectorizer()),('clf',MultinomialNB())])\n",
    "X_train = X_train['title'].values.ravel()\n",
    "text_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39e4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7887056763461258\n"
     ]
    }
   ],
   "source": [
    "accuracy = text_clf.score(X_test['title'], y_test)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286f37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dayOfWeek     2.165008e-42\n",
       "categoryId    0.000000e+00\n",
       "views         0.000000e+00\n",
       "likes         0.000000e+00\n",
       "comments      0.000000e+00\n",
       "duration      0.000000e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Categorical Feature(Chi-Squared Test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state=123)\n",
    "\n",
    "f_score=chi2(X_train,y_train)\n",
    "\n",
    "pvalues = pd.Series(f_score[1])\n",
    "pvalues.index = X_train.columns\n",
    "pvalues.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4c279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['likes', 'comments'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Regularization-based Feature Selection(L1 (Lasso) / L2(Ridge))\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state=123)\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "sel = SelectFromModel(LogisticRegression(penalty=\"l2\",C=1,solver=\"liblinear\"))\n",
    "\n",
    "sel.fit(X_train,y_train)\n",
    "\n",
    "sel.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb19f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "duration        0.059012\n",
       "categoryId      0.044644\n",
       "title_length    0.022751\n",
       "dayOfWeek       0.005229\n",
       "daytime         0.004172\n",
       "dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mutual Information\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df[['categoryId',  'dayOfWeek','daytime','title_length','duration']]\n",
    "y = df['trendingOrNot']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state=123)\n",
    "mutual_info = mutual_info_classif(X_train,y_train)\n",
    "mutual_info = pd.Series(mutual_info)\n",
    "mutual_info.index = X_train.columns\n",
    "mutual_info.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bafcd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFE(estimator=DecisionTreeClassifier(), n_features_to_select=2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recursive Feature Elimination(RFE)\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = df[['categoryId',  'dayOfWeek','daytime','title_length','duration']]\n",
    "y = df['trendingOrNot']\n",
    "\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(),n_features_to_select=2)\n",
    "rfe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5dca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categoryId selected=False rank=2\n",
      "dayOfWeek selected=False rank=3\n",
      "daytime selected=False rank=4\n",
      "title_length selected=True rank=1\n",
      "duration selected=True rank=1\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(X.columns):\n",
    "    print(f\"{col} selected={rfe.support_[i]} rank={rfe.ranking_[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
